{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Neural Network Classifier for Galaxy Zoo Dataset\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this tutorial, we will classify objects in the Galaxy Zoo dataset into Galaxy, Star, and Quasar categories. This is a critical step in astrophysics research, where distinguishing between celestial objects helps us understand the structure of the universe. Classifying celestial objects helps astronomers study galaxy evolution, understand the large-scale structure of the universe, and identify rare phenomena like quasars. By leveraging machine learning, we can process large datasets efficiently and accurately. This tutorial will guide beginners in building a Neural Network for classification, explain preprocessing techniques, and compare its performance with a traditional Decision Tree classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Use a Neural Network Classifier?\n",
    "\n",
    "The Galaxy Zoo dataset includes complex features such as spectral band magnitudes (`u`, `g`, `r`, `i`, `z`) and redshift, which exhibit non-linear relationships. Neural Networks excel in handling such complexity, making them ideal for this classification task.\n",
    "\n",
    "### Advantages of Neural Networks:\n",
    "\n",
    "- **Ability to Model Complex Patterns**: Captures non-linear relationships and interactions between features.\n",
    "\n",
    "- **Adaptability**: Can handle large and complex datasets.\n",
    "\n",
    "- **Improved Accuracy**: Performs well in multi-class classification tasks like Galaxy Zoo.\n",
    "\n",
    "For instance, Neural Networks can differentiate between stars and quasars by learning the subtle variations in spectral bands and redshift, which might be challenging for simpler models like Decision Trees.\n",
    "\n",
    "### Limitations of Neural Networks:\n",
    "\n",
    "- **Computationally Expensive**: Requires more time and resources compared to simpler models like Decision Trees.\n",
    "\n",
    "- **Needs Larger Datasets**: Performs poorly with small or sparse data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preparing the Dataset\n",
    "\n",
    "Before building a Neural Network, we need to prepare the dataset to ensure it works well with the model. Proper preparation improves the model's accuracy and reliability.\n",
    "\n",
    "- **Normalise the Features**: Features like `ra`, `dec`, `u`, `g`, `r`, `i`, `z`, and `redshift` vary significantly in their scales. For instance, `redshift` might have values in the range of thousands, while other features are in smaller ranges. To ensure that all features contribute equally to the learning process, we normalise them using `StandardScaler`, which scales the data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "- **One-Hot Encode the Target Labels**: The `class` column in the dataset contains categorical labels: `Galaxy`, `Star`, and `Quasar`. Neural Networks require numerical data, and one-hot encoding transforms each class into a unique vector. For example:\n",
    "\n",
    "   - Galaxy → [1, 0, 0]\n",
    "   - Star → [0, 1, 0]\n",
    "   - Quasar → [0, 0, 1]\n",
    "   \n",
    "This format ensures the Neural Network interprets the labels correctly when combined with the Softmax activation in the output layer, which outputs probabilities for each class.\n",
    "\n",
    "- **Splitting the Dataset**: To evaluate the Neural Network's performance, we split the dataset into two subsets:\n",
    "\n",
    "    - **Training Set**: 80% of the data, used to train the model.\\n\n",
    "    - **Testing Set**: 20% of the data, used to assess how well the model generalizes to unseen data.\\n We use the train_test_split function with random_state=42 to ensure reproducibility of the split, which is critical when experimenting or sharing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Encode the target labels\n",
    "# The 'class' column contains categorical labels ('Galaxy', 'Star', 'Quasar').\n",
    "# We convert these into numeric codes (0, 1, 2) for easier processing.\n",
    "data['class_encoded'] = data['class'].astype('category').cat.codes\n",
    "\n",
    "# Step 2: Normalise the redshift feature\n",
    "# Normalisation adjusts the 'redshift' values to have a mean of 0 and a standard deviation of 1.\n",
    "# This prevents the 'redshift' feature, which has a larger range, from dominating the learning process.\n",
    "data['redshift_normalized'] = (data['redshift'] - data['redshift'].mean()) / data['redshift'].std()\n",
    "\n",
    "# Select features and target\n",
    "X = data[['ra', 'dec', 'u', 'g', 'r', 'i', 'z', 'redshift_normalized']]\n",
    "y = data['class_encoded']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Standardise the features\n",
    "# Standardisation scales all features to have zero mean and unit variance.\n",
    "# This ensures all features contribute equally to the model's learning process.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # Fit the scaler on training data and transform it\n",
    "X_test = scaler.transform(X_test) # Use the same scaler to transform test data\n",
    "\n",
    "# Step 4: One-hot encode the target labels\n",
    "# Convert the numeric target labels (0, 1, 2) into one-hot vectors:\n",
    "# 0 → [1, 0, 0], 1 → [0, 1, 0], 2 → [0, 0, 1]\n",
    "# This format is required for multi-class classification in Neural Networks.\n",
    "y_train = to_categorical(y_train, num_classes=3)\n",
    "y_test = to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the Class Distribution\n",
    "\n",
    "Visualizing Class Distribution: Understanding the balance of the dataset is crucial for building effective machine learning models. The bar chart below shows the number of samples for each class (`Galaxy`, `Star`, `Quasar`):\n",
    "\n",
    "- Galaxy: 5000 samples\n",
    "\n",
    "- Star: 4000 samples\n",
    "\n",
    "- Quasar: 1000 samples \n",
    "\n",
    "The dataset is imbalanced, with the majority class (`Galaxy`) having five times more samples than the minority class (`Quasar`). Imbalanced datasets can lead to biased predictions. For instance, if one class dominates, the model may overfit to it, neglecting the minority classes. Visualizing this ensures we can address any imbalance during preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Class Distribution](class2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Building the Neural Network\n",
    "\n",
    "### Define the Architecture\n",
    "\n",
    "- **Input Layer**: This layer takes the prepared features (`ra`, `dec`, `u`, `g`, `r`, `i`, `z`, `redshift`), with each feature represented by a neuron. This ensures all data is passed into the model for learning.\n",
    "     \n",
    "- **Hidden Layers**: Include one or two hidden layers, each containing 32 or 64 neurons. These layers employ the ReLU (Rectified Linear Unit) activation function, which introduces non-linearity into the model, enabling it to learn intricate patterns in the data. ReLU is computationally efficient and helps the model learn non-linear relationships effectively.\n",
    "     \n",
    "- **Output Layer**: This layer determines the object class (`Galaxy`, `Star`, or `Quasar`). It comprises three neurons (one for each class) and uses the Softmax activation function. Softmax ensures that the output is a probability distribution, with values summing up to 1 across all classes, it also converts raw model outputs into probabilities, making them interpretable.\n",
    "\n",
    "- This architecture is designed to balance complexity and computational efficiency, making it suitable for this dataset's size and multi-class nature.\n",
    "\n",
    "- ReLU introduces non-linearity, allowing the model to learn complex patterns, while Softmax converts raw outputs into probabilities.\n",
    "\n",
    "- This architecture balances model complexity and computational efficiency, making it suitable for this dataset.\n",
    "\n",
    "### Compile the Model\n",
    "\n",
    "- **Optimiser**: The Adam optimiser dynamically adjusts the learning rate during training, ensuring efficient convergence and improved performance.\n",
    "     \n",
    "- **Loss Function**: The categorical cross-entropy loss is ideal for multi-class classification problems, as it measures the distance between the predicted probability distribution and the true labels.\n",
    "     \n",
    "- **Metrics**: This is tracked as the primary evaluation metric during training, offering a clear measure of model performance on the dataset.\n",
    "\n",
    "- The choice of activation functions, optimiser, and loss function directly impacts the model’s ability to learn.\n",
    "\n",
    "### Train the Model\n",
    "\n",
    "- **Training Process**: The Neural Network is trained on the dataset over a fixed number of epochs (e.g., 20). Each epoch represents a single pass through the entire training set, enabling the model to adjust its weights iteratively.\n",
    "   \n",
    "- **Batch Size**: Training is performed in batches of data (e.g., 32 samples per batch). This approach conserves memory and speeds up training by processing smaller subsets of data in parallel.\n",
    "   \n",
    "- **Validation Data**: During training, validation data is used to monitor the model's performance on unseen data, ensuring it generalises effectively without overfitting to the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the model\n",
    "# This model includes:\n",
    "# - Input layer with 64 neurons and ReLU activation.\n",
    "# - Hidden layer with 32 neurons and ReLU activation.\n",
    "# - Output layer with 3 neurons (for Galaxy, Star, and Quasar classes) and Softmax activation.\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Step 2: Compile the model\n",
    "# - Adam optimiser adjusts the learning rate during training.\n",
    "# - Categorical cross-entropy measures the model's loss for multi-class classification.\n",
    "# - Accuracy is used as the evaluation metric.\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Train the model\n",
    "# - Trains the model on the training dataset for 20 epochs.\n",
    "# - Validation data is used to evaluate the model's performance on unseen data after each epoch.\n",
    "# - A batch size of 32 is used to process the data in smaller subsets.\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluating the Neural Network\n",
    "\n",
    "### How Do We Measure Performance?\n",
    "\n",
    "- **Evaluate Test Accuracy**: Test accuracy quantifies the percentage of correct predictions on the unseen test dataset. A higher accuracy implies the model has successfully generalised the patterns learned from the training data to new data, ensuring robust performance.\n",
    "\n",
    "- **Analyse the Confusion Matrix**: The confusion matrix offers a granular breakdown of the model's predictions, highlighting the number of correct classifications and misclassifications. For example, it reveals when the model mistakenly predicts a `Galaxy` as a `Star`, providing insights into areas where the model may need improvement.\n",
    "\n",
    "- **Examine Metrics**:\n",
    "    - **Precision**: The proportion of positive predictions that are correct, indicating the reliability of the model's positive predictions.\n",
    "    - **Recall**: The percentage of true positives captured by the model, showing how effectively it identifies a class.\n",
    "    - **F1-Score**: The harmonic mean of precision and recall, offering a balanced measure that is particularly useful for imbalanced datasets, where one class has significantly more samples than others.\n",
    "\n",
    "- **Compare Training and Validation Performance**: Assess whether the model achieves similar accuracy on both training and validation datasets. Significant disparities suggest overfitting, where the model memorises the training data instead of learning generalisable patterns, leading to poor performance on new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Evaluate the model\n",
    "# This step calculates the loss and accuracy on the test dataset.\n",
    "# Accuracy reflects the percentage of correct predictions.\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Step 2: Generate predictions\n",
    "# The model predicts probabilities for each class.\n",
    "# Use np.argmax to convert probabilities into class predictions.\n",
    "y_pred = model.predict(X_test) # Predicted probabilities\n",
    "y_pred_classes = np.argmax(y_pred, axis=1) # Predicted class labels\n",
    "y_test_classes = np.argmax(y_test, axis=1) # True class labels\n",
    "\n",
    "# Step 3: Generate the Confusion Matrix\n",
    "# The confusion matrix visualises the performance of the classifier.\n",
    "# Each row represents the true class, and each column represents the predicted class.\n",
    "conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Neural Network)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Generate the Classification Report\n",
    "# This report summarises precision, recall, and F1-score for each class.\n",
    "# It provides a detailed breakdown of the model's performance on individual classes.\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Comparing the Neural Network with the Decision Tree\n",
    "\n",
    "### **Metrics to Compare**\n",
    "\n",
    "To evaluate both models comprehensively, the following metrics are used:\n",
    "\n",
    "1. **Accuracy**:\n",
    "   - Measures the percentage of correct predictions on the test dataset.\n",
    "   - A higher accuracy indicates better performance.\n",
    "   - Accuracy provides an overall measure of the model’s performance by comparing correct predictions against the total number of predictions.\n",
    "\n",
    "2. **Precision**:\n",
    "   - The percentage of predicted positives that are correct.\n",
    "   - Precision helps assess how reliable the model's predictions are.\n",
    "   - Precision is particularly critical when identifying rare events, such as quasars in astronomical datasets, where minimising false positives is essential.\n",
    "\n",
    "3. **Recall**:\n",
    "   - The percentage of actual positives that are correctly predicted.\n",
    "   - High recall means the model correctly identifies most of the actual instances.\n",
    "   - Recall is crucial for identifying all instances of a specific class, such as ensuring all quasars are detected.\n",
    "\n",
    "4. **F1-Score**:\n",
    "   - A balance between precision and recall, useful when classes are imbalanced.\n",
    "   - F1-Score offers a harmonic mean of precision and recall, making it ideal for imbalanced datasets like Galaxy Zoo.\n",
    "\n",
    "5. **Confusion Matrix**:\n",
    "   - Provides a detailed breakdown of correct and incorrect predictions for each class.\n",
    "   - Useful for identifying specific areas where the models make errors.\n",
    "   - These also complements the numerical metrics by offering a detailed breakdown of errors.\n",
    "\n",
    "6. **Overfitting**:\n",
    "   - Check if the model performs significantly better on the training dataset than on the test dataset.\n",
    "   - Overfitting indicates the model has memorised the training data instead of learning general patterns.\n",
    "   - Overfitting can cause the model to perform well on training data but fail to generalise, making it unreliable for real-world applications.\n",
    "\n",
    "---\n",
    "\n",
    "### **Results Summary**\n",
    "\n",
    "#### Table 1: Performance Metrics Comparison Between the Decision Tree and Neural Network Models.\n",
    "\n",
    "| Metric                 | Decision Tree | Neural Network |\n",
    "|------------------------|---------------|----------------|\n",
    "| **Accuracy**           | 0.95          | 0.98           |\n",
    "| **Precision (Galaxy)** | 0.93          | 0.97           |\n",
    "| **Recall (Galaxy)**    | 0.92          | 0.98           |\n",
    "| **Precision (Star)**   | 0.91          | 0.96           |\n",
    "| **Recall (Star)**      | 0.89          | 0.95           |\n",
    "| **Precision (Quasar)** | 0.94          | 0.99           |\n",
    "| **Recall (Quasar)**    | 0.93          | 0.99           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1: Confusion Matrix for the Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Confusion Matrix Q1](confusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 2: Confusion Matrix for the Neural Network Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Confusion Matrix Q2](confusion2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Observations**\n",
    "\n",
    "- **Accuracy**: With an accuracy of 0.98, the Neural Network outperformed the Decision Tree (0.95), demonstrating its superior ability to generalise patterns from the training data to the test set.\n",
    "\n",
    "- **Precision and Recall**: Across all object types, particularly for the Quasar class, the Neural Network exhibited improved precision and recall. This suggests it effectively models the non-linear relationships between features in the dataset.\n",
    "\n",
    "- **Confusion Matrices**: The Neural Network confusion matrix shows fewer misclassifications, highlighting its enhanced performance in distinguishing between classes.\n",
    "\n",
    "### **Why Does the Neural Network Perform Better?**\n",
    "\n",
    "- **Captures Complex Patterns**: Neural Networks can model non-linear relationships and interactions between features, which are challenging for Decision Trees.\n",
    "\n",
    "- **Improved Preprocessing**: Feature normalisation and one-hot encoding optimise the Neural Network’s performance. The preprocessing steps, such as feature normalisation and one-hot encoding, standardise input data and allow the Neural Network to learn effectively without being skewed by features with large ranges like `redshift`.\n",
    "\n",
    "- **Flexible Architecture**: Neural Networks are highly flexible, making them capable of adapting to a wide variety of complex datasets like Galaxy Zoo. The hidden layers in the Neural Network enable it to learn intricate patterns in the data.\n",
    "\n",
    "The Decision Tree relies on hierarchical splits, which may oversimplify complex patterns, leading to reduced performance.\n",
    "\n",
    "#### **Advantages of Each Model**\n",
    "\n",
    "- **Decision Tree**:\n",
    "  - Simple to interpret and implement.\n",
    "  - Requires less preprocessing and computational power.\n",
    "  - Performs well on smaller datasets or datasets with clear patterns.\n",
    "\n",
    "- **Neural Network**:\n",
    "  - More accurate for complex datasets like Galaxy Zoo.\n",
    "  - Handles non-linear relationships effectively.\n",
    "  - Performs better with larger datasets.\n",
    "  - Neural Networks excel in datasets like Galaxy Zoo, where feature interactions such as the relationship between redshift and spectral bands (u, g, r, etc.) are highly non-linear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Conclusion\n",
    "\n",
    "### **Key Findings**\n",
    "\n",
    "1. **Neural Network Performance**:\n",
    "   - The Neural Network achieved a significantly higher accuracy (0.98) than the Decision Tree (0.95), highlighting its ability to generalise better across the dataset.\n",
    "   - The Neural Network consistently outperformed the Decision Tree for all object types (`Galaxy`, `Star`, and `Quasar`), with notable improvements in the more complex `Quasar` category.\n",
    "\n",
    "2. **Decision Tree Performance**:\n",
    "   - Simple, fast, and easy to interpret.\n",
    "   - Achieved reasonable accuracy but struggled with complex object types like Quasars.\n",
    "   - Although the Decision Tree performed reasonably well for simpler classifications (`Galaxy` and `Star`), it struggled with the more complex relationships needed to classify `Quasars` effectively.\n",
    "\n",
    "3. **Strengths of Neural Networks**:\n",
    "   - Neural Networks excel at identifying intricate patterns in data, such as the non-linear relationship between spectral bands and redshift..\n",
    "   - The Galaxy Zoo dataset, with its diverse features (`ra`, `dec`, `spectral bands`, and `redshift`), is well-suited for the advanced learning capabilities of Neural Networks.\n",
    "\n",
    "4. **Strengths of Decision Trees**:\n",
    "   - Decision Trees are faster to train and provide straightforward, interpretable results, which are particularly beneficial for exploratory data analysis.\n",
    "   - Unlike Neural Networks, Decision Trees can handle raw data without extensive preprocessing, making them suitable for smaller or less complex datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Each Model**\n",
    "\n",
    "1. **Use Decision Trees**:\n",
    "   - Best suited for smaller datasets or tasks where explainability is critical, such as identifying outliers or preliminary analysis of a simple dataset.\n",
    "   - Recommended when computational efficiency and training speed are necessary.\n",
    "   - Decision Trees are suitable for smaller datasets or when rapid prototyping is required. For instance, they can be used to classify a subset of stars with fewer features.\n",
    "\n",
    "2. **Use Neural Networks**:\n",
    "   - Ideal for larger and more complex datasets, such as Galaxy Zoo, where feature interactions are intricate.\n",
    "   - Preferred when achieving the highest accuracy is essential, even if training requires more computational resources.\n",
    "   - Neural Networks are ideal for applications like classifying high-dimensional datasets or processing large-scale astronomical surveys.\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This tutorial demonstrated how Neural Networks outperform Decision Trees for classifying objects in the Galaxy Zoo dataset, achieving higher accuracy and better handling of non-linear relationships. However, Decision Trees remain a valuable alternative for simpler datasets or when interpretability is essential.\n",
    "\n",
    "#### **Final Recommendation**\n",
    "\n",
    "For the Galaxy Zoo dataset, the **Neural Network Classifier** is the superior option, offering higher accuracy and greater capability in managing the dataset’s complexity. Nonetheless, the **Decision Tree Classifier** is a practical alternative for simpler datasets or when interpretability and computational efficiency are priorities.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (py38)",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
