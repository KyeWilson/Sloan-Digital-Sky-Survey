{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Decision Tree Classifier for Galaxy Zoo Dataset\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Machine learning can be used to classify celestial objects based on their properties. In this tutorial, we explore how to use a **traditional machine learning approach** to classify objects from the Galaxy Zoo dataset into three categories: **Galaxy**, **Star**, and **Quasar**. We will use a **Decision Tree Classifier**, a simple yet effective method for classification problems.\n",
    "\n",
    "This guide is designed for **beginners in Machine Learning** who are familiar with Python but new to ML concepts. We will walk through the steps necessary to replicate our analysis\n",
    "\n",
    "## Why Use a Decision Tree Classifier?\n",
    "\n",
    "Decision Trees are a popular choice for traditional machine learning tasks due to several advantages:\n",
    "\n",
    "- **Interpretability**: The structure of a decision tree is easy to understand and interpret, even for non-technical stakeholders.\n",
    "\n",
    "- **Handles Mixed Data Types**: Decision Trees can handle both numerical and categorical features without requiring extensive preprocessing.\n",
    "\n",
    "- **Non-Linear Relationships**: They can capture non-linear relationships between features and the target variable.\n",
    "\n",
    "### Limitations of Decision Trees\n",
    "\n",
    "Despite their advantages, Decision Trees have some limitations:\n",
    "\n",
    "- **Tendency to Overfit**: They can become overly complex and memorize the training data, resulting in poor generalization to unseen data.\n",
    "\n",
    "- **High Variance**: Small changes in the training data can lead to entirely different trees being generated.\n",
    "\n",
    "- **Less Robust**: They may perform worse compared to ensemble methods like Random Forest or Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started with Jupyter Notebook\n",
    "- A **Jupyter Notebook** is an interactive environment where you can write and run code in small chunks called \"cells.\"\n",
    "- Types of cells:\n",
    "  1. **Code cells**: For writing Python code.\n",
    "  2. **Markdown cells**: For headings, explanations, and instructions (like this one).\n",
    "- To run a cell:\n",
    "  1. Click on the cell.\n",
    "  2. Press `Shift + Enter`.\n",
    "\n",
    "---\n",
    "\n",
    "To complete this tutorial, you will need:\n",
    "1. Python installed on your computer.\n",
    "2. The Galaxy Zoo dataset saved in the same folder as this notebook.\n",
    "\n",
    "Let’s get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Explore the Dataset\n",
    "\n",
    "### What is the Galaxy Zoo Dataset?\n",
    "\n",
    "The **Galaxy Zoo dataset** contains astronomical object classifications based on images taken from telescopes. Key features include:\n",
    "\n",
    "- **ra (Right Ascension) & dec (Declination)**: Coordinates of objects in the sky.\n",
    "\n",
    "- **u, g, r, i, z filters**: Measurements of light intensity at different wavelengths.\n",
    "\n",
    "- **redshift**: A measure of how much the object’s light has been stretched due to the expansion of the universe.\n",
    "\n",
    "- **class**: The category of the object (Galaxy, Star, Quasar).\n",
    "\n",
    "### How to Load the Data\n",
    "\n",
    "To start, load the dataset using the `pandas` library. The first few rows of the dataset can be displayed to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"galaxy_zoo.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure data quality, check for **missing values** and understand the distribution of object classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(data.isnull().sum()) \n",
    "# Distribution of target classes\n",
    "print(data['class'].value_counts()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Target Class Distribution\n",
    "\n",
    "We can plot the distribution of object classes to understand the dataset’s balance. here's an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Target Class Distribution](class.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing\n",
    "\n",
    "### Why is Preprocessing Important?\n",
    "\n",
    "Raw data often contains **irrelevant features, missing values, and inconsistencies**. Preprocessing ensures that our model learns from meaningful data.\n",
    "\n",
    "### Key Preprocessing Steps\n",
    "\n",
    "1. **Drop Unnecessary Columns** – Some columns do not contribute to classification and should be removed.\n",
    "\n",
    "2. **Encode Categorical Data** – Convert the `class` column into numerical values for machine learning.\n",
    "\n",
    "3. **Normalise the Redshift Feature** – Standardising numerical values helps models learn efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['objid', 'specobjid', 'fiberid', 'plate', 'mjd'])\n",
    "\n",
    "# Encode 'class' column\n",
    "data['class_encoded'] = data['class'].astype('category').cat.codes\n",
    "\n",
    "# Normalise redshift\n",
    "data['redshift_normalised'] = (data['redshift'] - data['redshift'].mean()) / data['redshift'].std()\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Redshift Distribution\n",
    "\n",
    "We can visualise the redshift feature to understand its range and distribution. Here is an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Redshift Distribution](redshift.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Selection and Training\n",
    "\n",
    "### Why Use a Decision Tree Classifier?\n",
    "\n",
    "Decision Trees are **easy to interpret** and **work well with structured tabular data**. They split data into smaller subgroups using feature thresholds, creating a tree-like model of decisions.\n",
    "\n",
    "### Steps to Train the Model\n",
    "1. **Split the data**: Separate the dataset into training and testing subsets.\n",
    "\n",
    "2. **Train the model**: Fit a Decision Tree Classifier to the training data.\n",
    "\n",
    "3. **Tune hyperparameters**: Adjust tree depth to balance accuracy and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = data.drop(columns=['class', 'class_encoded'])\n",
    "y = data['class_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "model = DecisionTreeClassifier(max_depth=5)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Evaluation\n",
    "\n",
    "### How Do We Measure Performance?\n",
    "\n",
    "To assess model performance, use:\n",
    "\n",
    "1. **Accuracy Score**: Measures how many predictions were correct.\n",
    "\n",
    "2. **Confusion Matrix**: Shows the breakdown of correct vs. incorrect classifications.\n",
    "\n",
    "3. **Classification Report**: Provides precision, recall, and F1-score for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the Confusion Matrix\n",
    "\n",
    "A heatmap of the confusion matrix provides a clear visual representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Confusion Matrix](confusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Metrics\n",
    "\n",
    "- **Precision**: The proportion of true positive predictions among all positive predictions made by the model.\n",
    "\n",
    "- **Recall**: The proportion of true positives identified out of all actual positives.\n",
    "\n",
    "- **F1-Score**: The harmonic mean of precision and recall, balancing both metrics.\n",
    "\n",
    "These metrics are particularly important in imbalanced datasets where accuracy alone might be misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Hyperparameter Tuning\n",
    "\n",
    "### Testing Different Tree Depths\n",
    "\n",
    "Experiment with different values of `max_depth` to observe its effect on model accuracy.\n",
    "\n",
    "A deeper tree increases accuracy on training data but risks overfitting, making it less generalisable to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test varying tree depths\n",
    "for depth in range(1, 11):\n",
    "    model = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(f\"Tree Depth: {depth}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Conclusion and Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Traditional ML models like Decision Trees** can effectively classify astronomical objects.\n",
    "\n",
    "**Feature preprocessing** (dropping irrelevant columns, encoding, and normalisation) is crucial for improving accuracy.\n",
    "\n",
    "**Model evaluation metrics** help determine areas for improvement.\n",
    "\n",
    "### Summary\n",
    "\n",
    "This Jupyter Notebook guides you through a **step-by-step approach** to classifying celestial objects using a **traditional ML model**. By following these steps, a beginner can replicate the process and build a foundation in machine learning for astronomy.\n",
    "\n",
    "For more advanced techniques, check out **neural network approaches** (covered in Q2). 🚀\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R 4.0.3",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
